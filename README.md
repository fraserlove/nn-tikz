# Deep Learning and Neural Network Diagrams

A collection of TikZ diagrams for deep learning and neural network concepts. This repository was created to provide a set of open-source diagrams for use in academic papers, presentations, and other documents. The diagrams are designed to be easily customisable and can be used in LaTeX documents. Note that this repository is a work in progress and more diagrams will be added over time.

## Examples

#### Transformer - [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762)
<img src="assets/transformer.png" alt="Transformer" width="600"/>

#### Multi-Head Attention - [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762)
<img src="assets/multihead_attention.png" alt="Multi-Head Attention" width="600"/>

#### Neural Network
<img src="assets/neural_network.png" alt="Neural Network" width="600"/>

#### Attention Mechanism - [Bahdanau et al. (2014)](https://arxiv.org/abs/1409.0473)
<img src="assets/attention.png" alt="Attention Mechanism" width="600"/>

#### Gated Recurrent Unit (GRU) - [Cho et al. (2014)](https://arxiv.org/abs/1406.1078)
<img src="assets/gru.png" alt="GRU" width="600"/>

#### RNN Encoder-Decoder - [Sutskever et al. (2014)](https://arxiv.org/abs/1409.3215)
<img src="assets/rnn_encoder_decoder_sutskever.png" alt="RNN Encoder-Decoder" width="600"/>

#### Backpropagation Through Time (BPTT) - [Werbos (1990)](https://www.researchgate.net/publication/220365479_Backpropagation_through_time)
<img src="assets/rnn_backprop.png" alt="BPTT" width="600"/>

## Citation

If you use this repository in your research or project, please cite it as follows:
```
@misc{love2024nntikz,
    author = {Fraser Love},
    title = {TikZ Diagrams for Deep Learning and Neural Networks},
    year = 2024,
    url = {https://github.com/fraserlove/nntikz},
    note = {GitHub repository}
}
```